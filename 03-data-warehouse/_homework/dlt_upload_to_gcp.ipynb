{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC2QnhmKxpq1"
   },
   "source": [
    "**Using GCP ADC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UsUZobVduL7l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"DESTINATION__CREDENTIALS\"] = userdata.get(\"GCP_CREDENTIALS\")\n",
    "os.environ[\"BUCKET_URL\"] = \"gs://de-zc-pmg-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Credentials found at: /home/codespace/.config/gcloud/application_default_credentials.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dlt\n",
    "from dlt.destinations import filesystem\n",
    "\n",
    "# 1. Point to the credentials file generated by 'gcloud auth application-default login'\n",
    "# In Codespaces, this is the standard location:\n",
    "creds_path = \"/home/codespace/.config/gcloud/application_default_credentials.json\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = creds_path\n",
    "\n",
    "# 2. Verify the file actually exists (sanity check)\n",
    "if not os.path.exists(creds_path):\n",
    "    print(\"‚ö†Ô∏è WARNING: Credentials file not found! Did you run 'gcloud auth application-default login'?\")\n",
    "else:\n",
    "    print(f\"‚úÖ Credentials found at: {creds_path}\")\n",
    "\n",
    "# 3. Set your bucket URL\n",
    "MY_BUCKET = \"gs://de-zc-pmg-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mPBzsEgyjsBo"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install for production\n",
    "uv install dlt[bigquery, gs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "evdUsDNbkCTk"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install for testing\n",
    "!pip install dlt[duckdb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lYh7r1mTf4uo"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dlt.destinations import filesystem\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76zT1PzAgs7A"
   },
   "source": [
    "Ingesting parquet files to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xya0215jsnsb"
   },
   "outputs": [],
   "source": [
    "# Define a dlt source to download and process Parquet files as resources\n",
    "@dlt.source(name=\"rides\")\n",
    "def download_parquet():\n",
    "    prefix = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata\"\n",
    "    for month in range(1, 7):\n",
    "        file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
    "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        df = pd.read_parquet(BytesIO(response.content))\n",
    "\n",
    "        # Return the dataframe as a dlt resource for ingestion\n",
    "        yield dlt.resource(df, name=file_name)\n",
    "\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rides_pipeline\",\n",
    "    destination=filesystem(bucket_url=os.environ[\"BUCKET_URL\"], layout=\"{schema_name}/{table_name}.{ext}\"),\n",
    "    dataset_name=\"rides_dataset\",\n",
    ")\n",
    "\n",
    "# Run the pipeline to load Parquet data into DuckDB\n",
    "load_info = pipeline.run(download_parquet(), loader_file_format=\"parquet\")\n",
    "\n",
    "# Print the results\n",
    "print(load_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing yellow_tripdata_2024-01.parquet...\n",
      "Processing yellow_tripdata_2024-02.parquet...\n",
      "Processing yellow_tripdata_2024-03.parquet...\n",
      "Processing yellow_tripdata_2024-04.parquet...\n",
      "Processing yellow_tripdata_2024-05.parquet...\n",
      "Processing yellow_tripdata_2024-06.parquet...\n",
      "Pipeline rides_pipeline load step completed in 1 minute and 3.44 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset rides_ny_taxi\n",
      "The filesystem destination used gs://de-zc-pmg-data location to store data\n",
      "Load package 1770726527.519993 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "import os\n",
    "from dlt.destinations import filesystem\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SETUP: AUTHENTICATION\n",
    "# ---------------------------------------------------------\n",
    "# 1. Point to the ADT file (Do not read it, just point to it)\n",
    "creds_path = os.path.expanduser(\"~/.config/gcloud/application_default_credentials.json\")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = creds_path\n",
    "\n",
    "# 2. CRITICAL: Set your Project ID explicitly\n",
    "# ADT keys don't always contain the project ID, causing gcsfs to fail.\n",
    "# REPLACE 'your-project-id-here' with your actual GCP Project ID (not the name)\n",
    "os.environ[\"GCP_PROJECT_ID\"] = \"project-c94cb82b-89ba-4659-98a\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"project-c94cb82b-89ba-4659-98a\" # Set both just to be safe\n",
    "\n",
    "# 3. Define Bucket\n",
    "MY_BUCKET = \"gs://de-zc-pmg-data\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PIPELINE\n",
    "# ---------------------------------------------------------\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rides_pipeline\",\n",
    "    destination=filesystem(\n",
    "        bucket_url=MY_BUCKET,\n",
    "        # REMOVED: credentials=... \n",
    "        # We let dlt/gcsfs find the Env Vars automatically\n",
    "        layout=\"{schema_name}/{table_name}.{ext}\"\n",
    "    ),\n",
    "    dataset_name=\"rides_ny_taxi\",\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# RUN\n",
    "# ---------------------------------------------------------\n",
    "# Use the file-safe download function from before\n",
    "# If you need that function again, let me know!\n",
    "try:\n",
    "    load_info = pipeline.run(download_parquet, loader_file_format=\"parquet\")\n",
    "    print(load_info)\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: {e}\")\n",
    "    print(\"\\nüîç TROUBLESHOOTING:\")\n",
    "    print(\"1. Did you replace 'your-project-id-here' with your actual Project ID?\")\n",
    "    print(f\"2. Does this file exist? {creds_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0310FT-gy_P"
   },
   "source": [
    "Ingesting data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_3K97w1c2v2",
    "outputId": "4b2d26bf-2814-46fa-f80d-7a2e17417a95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:47:53,606|[WARNING]|47028|127916415637312|dlt|pipeline.py|_state_to_props:1731|The destination dlt.destinations.filesystem:None in state differs from destination dlt.destinations.bigquery:bigquery in pipeline and will be ignored\n"
     ]
    }
   ],
   "source": [
    "# Define a dlt resource to download and process Parquet files as single table\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
    "def download_parquet():\n",
    "    prefix = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata'\n",
    "\n",
    "    for month in range(1, 7):\n",
    "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        df = pd.read_parquet(BytesIO(response.content))\n",
    "\n",
    "        yield df\n",
    "\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rides_pipeline\",\n",
    "    # destination=\"duckdb\",  # Use DuckDB for testing\n",
    "    destination=\"bigquery\",  # Use BigQuery for production\n",
    "    dataset_name=\"rides_dataset\",\n",
    ")\n",
    "\n",
    "# Run the pipeline to load Parquet data into DuckDB\n",
    "info = pipeline.run(download_parquet)\n",
    "\n",
    "# Print the results\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDcLjzLtooBV",
    "outputId": "74ff2de7-2f2e-41b9-a681-3dc5887f6eed"
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "\n",
    "# Set search path to the dataset\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "\n",
    "# Describe the dataset to see loaded tables\n",
    "res = conn.sql(\"DESCRIBE\").df()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVJy8JoerI2P",
    "outputId": "3f8c7fee-a9ee-4fd4-ec75-153ca60bd36f"
   },
   "outputs": [],
   "source": [
    "# provide a resource name to query a table of that name\n",
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(f\"SELECT count(1) FROM rides\") as cursor:\n",
    "        data = cursor.df()\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
